{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuidoGiacomoMussini/SocialNetworkAnalysis--Algorithm/blob/main/SNA_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkFdmptzcPWJ"
      },
      "source": [
        "# **0 - Initialistion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydJXq903cpDg"
      },
      "source": [
        "## 0.1 - Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hVsNHm_ZkU6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import random as rnd\n",
        "from IPython.display import clear_output \n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from collections import defaultdict, Counter, OrderedDict\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import networkx as nx\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdnsvT8IctpR"
      },
      "source": [
        "## 0.2 - Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yQRuVn3I6Rvs"
      },
      "outputs": [],
      "source": [
        "def re_index(data):\n",
        "  s = pd.Series(range(len(data)))\n",
        "  data = data.set_index(s)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6eyyfQO63efm"
      },
      "outputs": [],
      "source": [
        "def tensor_mean(tens):\n",
        "  ex = tens.numpy()\n",
        "  colmean = ex.mean(axis=1)\n",
        "  totmean = colmean.mean()\n",
        "  return totmean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ag0LndyGpKW_"
      },
      "outputs": [],
      "source": [
        "#https://discuss.pytorch.org/t/how-to-show-a-image-in-jupyter-notebook-with-pytorch-easily/1229/4?u=ataraxy\n",
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_definition(net_train, y_train):\n",
        "  links_list, nodes_list, density_list, centrality_list, clust_list, transitivity_list, label = [], [], [], [], [], [], []\n",
        "\n",
        "  for i in tqdm_notebook(range(len(net_train))):\n",
        "    dic =nx.degree_centrality(net_train[i]) \n",
        "    dic = list(dic.values())\n",
        "    centrality_list.append(np.mean(dic))\n",
        "    links_list.append(net_train[i].number_of_edges())\n",
        "    nodes_list.append(net_train[i].number_of_nodes())\n",
        "    density_list.append(nx.density(net_train[i]))\n",
        "    clust_list.append(nx.average_clustering(net_train[i]))\n",
        "    transitivity_list.append(nx.transitivity(net_train[i]))\n",
        "\n",
        "  train_df = pd.DataFrame( columns = ['links', 'nodes', 'density', 'centrality', 'clustering', 'transitivity'])\n",
        "  train_df['links'] = links_list\n",
        "  train_df['nodes'] = nodes_list\n",
        "  train_df['density'] = density_list\n",
        "  train_df['centrality'] = centrality_list\n",
        "  train_df['clustering'] = clust_list\n",
        "  train_df['transitivity'] = transitivity_list\n",
        "  train_df['label'] = y_train\n",
        "  return train_df"
      ],
      "metadata": {
        "id": "ZluNY5wYPnh0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alg_mean(train_df, label1, label2):\n",
        "\n",
        "  avg_links_0, avg_nodes_0, avg_density_0, avg_centrality_0, avg_clust_0, avg_transitivity_0 = 0,0,0,0,0,0\n",
        "  avg_links_1, avg_nodes_1, avg_density_1, avg_centrality_1, avg_clust_1, avg_transitivity_1 = 0,0,0,0,0,0\n",
        "\n",
        "  avg_links_0 = train_df.loc[train_df['label'] == label1, 'links'].mean()\n",
        "  avg_nodes_0 = train_df.loc[train_df['label'] == label1, 'nodes'].mean()\n",
        "  avg_density_0 = train_df.loc[train_df['label'] == label1, 'density'].mean()\n",
        "  avg_centrality_0 = train_df.loc[train_df['label'] == label1, 'centrality'].mean()\n",
        "  avg_clust_0 = train_df.loc[train_df['label'] == label1, 'clustering'].mean()\n",
        "  avg_transitivity_0 = train_df.loc[train_df['label'] == label1, 'transitivity'].mean()\n",
        "\n",
        "  avg_links_1 = train_df.loc[train_df['label'] == label2, 'links'].mean()\n",
        "  avg_nodes_1 = train_df.loc[train_df['label'] == label2, 'nodes'].mean()\n",
        "  avg_density_1 = train_df.loc[train_df['label'] == label2, 'density'].mean()\n",
        "  avg_centrality_1 = train_df.loc[train_df['label'] == label2, 'centrality'].mean()\n",
        "  avg_clust_1 = train_df.loc[train_df['label'] == label2, 'clustering'].mean()\n",
        "  avg_transitivity_1 = train_df.loc[train_df['label'] == label2, 'transitivity'].mean()\n",
        "\n",
        "  tr_dist_0 = (avg_links_0, avg_nodes_0, avg_density_0, avg_centrality_0, avg_clust_0, avg_transitivity_0)\n",
        "  tr_dist_1 = (avg_links_1, avg_nodes_1, avg_density_1, avg_centrality_1, avg_clust_1, avg_transitivity_1)\n",
        "\n",
        "  return tr_dist_0, tr_dist_1"
      ],
      "metadata": {
        "id": "wlHOykW9Pseu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def array_to_tuple(test_df):\n",
        "  tpl_list = []\n",
        "\n",
        "  for i in range(len(test_df)):\n",
        "    tpl = test_df.loc[i]\n",
        "    tpl_list.append((tpl[0], tpl[1], tpl[2], tpl[3], tpl[4], tpl[5]))\n",
        "  return tpl_list"
      ],
      "metadata": {
        "id": "t7TufIFiPwp5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(tpl_list, label1, label2):\n",
        "  predicted = []\n",
        "  for i in range(len(tpl_list)): \n",
        "    one = math.dist(tr_dist_1, tpl_list[i])\n",
        "    zero = math.dist(tr_dist_0, tpl_list[i])\n",
        "    if one > zero: \n",
        "      predicted.append(label1)\n",
        "    else:\n",
        "      predicted.append(label2)\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "7QIg2G7vPxjI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DGUb17oECQ_"
      },
      "source": [
        "## 0.3 - Import the Data From Kaggle \n",
        "*(estimated time: 4 minutes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VERqEz_q_Vzz"
      },
      "outputs": [],
      "source": [
        "!mkdir WD\n",
        "\n",
        "! #mkdir ~/.kaggle\n",
        "! #cp kaggle.json ~/.kaggle/\n",
        "! #chmod 600 ~/.kaggle/kaggle.json\n",
        "os.environ['KAGGLE_USERNAME'] = \"guidomussini\"\n",
        "os.environ['KAGGLE_KEY'] = \"f7b24d630bc3e7e7fda7a5a1b32f4582\"\n",
        "! kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "! unzip /content/skin-cancer-mnist-ham10000.zip -d /content/WD\n",
        "\n",
        "#remove useless data \n",
        "shutil.rmtree('/content/WD/ham10000_images_part_1')\n",
        "shutil.rmtree('/content/WD/ham10000_images_part_2')\n",
        "! rm '/content/WD/hmnist_28_28_L.csv'\n",
        "! rm '/content/WD/hmnist_28_28_RGB.csv'\n",
        "! rm '/content/WD/hmnist_8_8_L.csv'\n",
        "! rm '/content/WD/hmnist_8_8_RGB.csv'\n",
        "! rm '/content/skin-cancer-mnist-ham10000.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfxsIBCEIje"
      },
      "source": [
        "# **1 - Dataset Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLnL3_d6EPKR"
      },
      "source": [
        "##1.1 - Merge the images from the 2 folders\n",
        "\n",
        "*   Code taken by: https://www.kaggle.com/code/sid321axn/step-wise-approach-cnn-model-77-0344-accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "m9_InBsY_Zu3"
      },
      "outputs": [],
      "source": [
        "#Merge the images of the 2 folders\n",
        "base_skin_dir = os.path.join('..', 'content/WD')\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(base_skin_dir, '*', '*.jpg'))}\n",
        "\n",
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ci7zPVEWiQ"
      },
      "source": [
        "create a column in which each row contain the path to a image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "na3AfpSe_dUD"
      },
      "outputs": [],
      "source": [
        "sdf = pd.read_csv(os.path.join(base_skin_dir, 'HAM10000_metadata.csv'))\n",
        "\n",
        "sdf['path'] = sdf['image_id'].map(imageid_path_dict.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE5m2sGVEg6O"
      },
      "source": [
        "## 1.2 - Metadata Handling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL5-DrjhEj_V"
      },
      "source": [
        "check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIfpdqRg_gS9",
        "outputId": "b3e46841-5f37-45b0-8911-dc067d93ae71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7470\n"
          ]
        }
      ],
      "source": [
        "sdf = sdf.drop_duplicates(subset=['lesion_id']) \n",
        "sdf = sdf.drop_duplicates(subset=['image_id']) \n",
        "\n",
        "print(sdf.shape[0]) #now i have 7470 lesions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtDyJ-AlEqHy"
      },
      "source": [
        "check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5htrKrBD_kh_",
        "outputId": "8533d940-ae9d-4210-921f-9a368fae6d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-NA data per columns:\n",
            " lesion_id        0\n",
            "image_id         0\n",
            "dx               0\n",
            "dx_type          0\n",
            "age             52\n",
            "sex              0\n",
            "localization     0\n",
            "path             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"number of non-NA data per columns:\\n\",sdf.isnull().sum())\n",
        "#some NA in age -> since that column will be removed from the dataset, i don't impute them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No8M26AlEwHj"
      },
      "source": [
        "## 1.3 - Create the binary label:\n",
        "\n",
        "\n",
        "*   **M**: If the lesion is Malignant (0)\n",
        "*   **B**: If the lesion is Benign (1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "chdaPX_F_leK"
      },
      "outputs": [],
      "source": [
        "#Define Benign and Malignant lesions\n",
        "Benign = [\"nv\", \"bkl\", \"vasc\", \"df\"]\n",
        "Malignant = [\"mel\", \"bcc\", \"akiec\"]\n",
        "m1 = sdf['dx'].isin(Benign)\n",
        "m2 = sdf['dx'].isin(Malignant)\n",
        "sdf['type'] = np.select([m1, m2], [\"B\", \"M\"], default=sdf['dx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "EK3wFiFnl-mg"
      },
      "outputs": [],
      "source": [
        "sdf['label'] = pd.Categorical(sdf['type']).codes\n",
        "sdf = sdf[['path', 'label']]\n",
        "\n",
        "#this function, defined in the section 'functions', fix the row-indeces of the dataset\n",
        "#since the download (and in the next steps the sampling), save them in a unconvinient manner\n",
        "sdf = re_index(sdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GPDIkGUdlAK"
      },
      "source": [
        "## 1.4 - Split the Dataset in train and test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "x1Ft0jagl_Qa"
      },
      "outputs": [],
      "source": [
        "#train\n",
        "train = sdf.sample(frac=0.7, random_state=19)\n",
        "#test\n",
        "test = sdf.drop(train.index)\n",
        "\n",
        "#re index the df\n",
        "train = re_index(train)\n",
        "test = re_index(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1QS7uAeP2E"
      },
      "source": [
        "## 1.5 - Explicit the images as tensor \n",
        "*(estimated time: 3 minutes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "W4KgzMucmDpK"
      },
      "outputs": [],
      "source": [
        "sh_x, sh_y = 64, 64 #define the dimensions of the images\n",
        "\n",
        "train['image'] = train['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "test['image'] = test['path'].map(lambda x: np.asarray(Image.open(x).resize((sh_x,sh_y))))\n",
        "\n",
        "#remove useless columns from the dataset\n",
        "train = train[['image', 'label']]\n",
        "test = test[['image', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVxGVBK6fgvy"
      },
      "outputs": [],
      "source": [
        "show(torch.tensor(train['image'][3]).permute(2,0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rzd05BBUih2"
      },
      "source": [
        "# **2 - Trasform the Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy8BmsOperTK"
      },
      "source": [
        "## 2.1 - Apply center crop and grayscale filter to the images\n",
        "\n",
        "\n",
        "*   Center Crop to focus only on the lesion\n",
        "*   Grayscale to reduce the tensor dimensionality, deleting the 'color' dimension. This passage is necessary to build the A matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOkIYZMGm2Yf"
      },
      "outputs": [],
      "source": [
        "crop_dim = int(round(sh_x*0.7, 0)) #define the reduction of dimensionality by cropping the images\n",
        "\n",
        "transform1 = T.CenterCrop(size= crop_dim)\n",
        "transform2 = T.Grayscale()\n",
        "x_train = train[\"image\"].map(lambda x: transform2(transform1(torch.tensor(x).permute(2,0,1))))\n",
        "x_test = test[\"image\"].map(lambda x: transform2(transform1(torch.tensor(x).permute(2,0,1))))\n",
        "\n",
        "y_train = train[['label']] \n",
        "y_test = test[['label']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAEf3_avTSZO"
      },
      "outputs": [],
      "source": [
        "show(x_train[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsTpBWEzgJCl"
      },
      "source": [
        "## 2.2 - Apply a step function to obtain white-black images\n",
        "*(estimated time: 17 minutes)*\n",
        "\n",
        "Threshold set equal to the averege color level of the given image (see 'tensor_mean()' function)\n",
        "\n",
        "*  if pixel > tensor_mean --> black (255)\n",
        "*  if pixel < tensor_mean --> white (0)\n",
        "\n",
        "this passage has a primarily visual purpose, in the next chunks the trasformation will be 1 if black, 0 if white\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtqbCuEZplb6"
      },
      "outputs": [],
      "source": [
        "#trasform x_ train in 255-0 form\n",
        "for tens in tqdm_notebook(range(len(x_train))): \n",
        "  ex = x_train[tens]\n",
        "  mean = tensor_mean(ex)\n",
        "  for h in range(ex.shape[1]): \n",
        "    for l in range(ex.shape[2]):\n",
        "\n",
        "      if h == l: \n",
        "        ex[0][h][l] = 0\n",
        "      elif ex[0][h][l] > mean: \n",
        "        ex[0][h][l] = 0 \n",
        "\n",
        "  x_train[tens] = ex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hQ8vOKdxHz9"
      },
      "outputs": [],
      "source": [
        "show(torch.tensor(x_train[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdBhj7pFliRR"
      },
      "source": [
        "same procedure applyed to the test set. \n",
        "\n",
        "*(estimated time: 7 miutes)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhChm41ZlZ1o"
      },
      "outputs": [],
      "source": [
        "#trasform x_test in 255-0 form\n",
        "for tens in tqdm_notebook(range(len(x_test))): \n",
        "  ex = x_test[tens]\n",
        "  mean = tensor_mean(ex)\n",
        "  for h in range(ex.shape[1]): \n",
        "    for l in range(ex.shape[2]):\n",
        "\n",
        "      if h == l: \n",
        "        ex[0][h][l] = 0\n",
        "      elif ex[0][h][l] > mean: \n",
        "        ex[0][h][l] = 0 \n",
        "\n",
        "  x_test[tens] = ex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjUprFsVh0JB"
      },
      "source": [
        "reshape the images to obtain 2D matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cU7Fpq02L16d"
      },
      "outputs": [],
      "source": [
        "for i in range(len(x_train)):\n",
        "  x_train[i] = x_train[i].reshape(crop_dim, crop_dim).numpy()\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  x_test[i] = x_test[i].reshape(crop_dim, crop_dim).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odbEFd4ctRlR"
      },
      "source": [
        "## 2.4 - Save the netowrks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f7IHZyn0cQ5"
      },
      "source": [
        "create the network lists for train and test set and remove isolated nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "z-DePFL-y6FB"
      },
      "outputs": [],
      "source": [
        "net_train = []\n",
        "net_test = []\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "  provnet = nx.from_numpy_array(x_train[i], parallel_edges=False, create_using=None)\n",
        "  provnet.remove_nodes_from(list(nx.isolates(provnet)))\n",
        "  net_train.append(provnet)\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  provnet = nx.from_numpy_array(x_test[i], parallel_edges=False, create_using=None)\n",
        "  provnet.remove_nodes_from(list(nx.isolates(provnet)))\n",
        "  net_test.append(provnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRt6RfbUP4rG"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(x_train[3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nx.draw(net_train[3])"
      ],
      "metadata": {
        "id": "eX_lwLV8cJX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObZKcF9r4OR"
      },
      "source": [
        "# **3 - The Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pZzStYydr96H"
      },
      "outputs": [],
      "source": [
        "# Number of Links --> net.number_of_nodes()\n",
        "# Number of Nodes --> net.number_of_edges()\n",
        "# Density --> density(net)\n",
        "# Normalized centrality mean(dict) | dict = nx.degree_centrality(net)\n",
        "# GLobal clustering coefficient\n",
        "# Transitivity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Create the Network-metrics dataset. \n",
        "\n",
        "*(estimated time: 3 minutes)*\n",
        "\n",
        "The metrics that will be calculated are:\n",
        "*   Number of nodes\n",
        "*   Number of links\n",
        "*   Density\n",
        "*   Centrality\n",
        "*   Average clustering coefficient\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lWgiuV17GSj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train set "
      ],
      "metadata": {
        "id": "pj_ZaeJmGypc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = metrics_definition(net_train, y_train)\n",
        "test_df = metrics_definition(net_test, y_train)"
      ],
      "metadata": {
        "id": "H_-H541fSXaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove NaN from the datasets"
      ],
      "metadata": {
        "id": "n3iYz6kLMV0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()"
      ],
      "metadata": {
        "id": "dl4Hwh1sLohv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - train the algorithm"
      ],
      "metadata": {
        "id": "YJQvuH0kQo4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_dist_0, tr_dist_1 = alg_mean(train_df, 0, 1)\n",
        "te_dist_0, te_dist_1 = alg_mean(test_df, 0, 1)"
      ],
      "metadata": {
        "id": "AFo5MtJiSUiR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transoform the results in tuples"
      ],
      "metadata": {
        "id": "3CNAqqXzQ2kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tpl_te_list = array_to_tuple(test_df)"
      ],
      "metadata": {
        "id": "eHsZRWFya9Le"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 - Prediction"
      ],
      "metadata": {
        "id": "rc4MwIS_Q7L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_te = prediction(tpl_te_list, 0, 1)"
      ],
      "metadata": {
        "id": "fV7Ytqe8b0NH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "djAweiFLROx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ytest = test_df['label']\n",
        "\n",
        "cnf_test = metrics.confusion_matrix(ytest, predict_te)\n",
        "report_te = metrics.classification_report(ytest, predict_te)\n",
        "\n",
        "\n",
        "print(\"----------------------------TEST--------------------------------\\n\")\n",
        "print(report_te, \"\\n Confusion matrix \\n\", cnf_test)\n"
      ],
      "metadata": {
        "id": "3002CScURPCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Synthetic images generator**\n",
        "\n",
        "generate synthetic images based on ER random graph theory"
      ],
      "metadata": {
        "id": "8EzKMWdpSKbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SN_random(mat):\n",
        "  colmean = mat.mean(axis=1)\n",
        "  mu = colmean.mean()\n",
        "\n",
        "  rnd = np.zeros((mat.shape[0],mat.shape[1]))\n",
        "  net = nx.from_numpy_array(mat, parallel_edges=False, create_using=None)\n",
        "  net.remove_nodes_from(list(nx.isolates(net)))\n",
        "  p_one = nx.density(net)\n",
        "  for i in range(mat.shape[0]):\n",
        "    for j in range(mat.shape[1]):\n",
        "\n",
        "      if mat[i][j] == 0:\n",
        "        rnd[i][j] == 0\n",
        "      elif p_one <= random.uniform(0, 1):\n",
        "        rnd[i][j] =random.randint(round(mat.max()-mu, 0), mat.max())\n",
        "      else: \n",
        "        rnd[i][j] = mat[i][j]\n",
        "\n",
        "  return rnd"
      ],
      "metadata": {
        "id": "_CKVZIR9SLGb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt1 = x_train[3]\n",
        "plt2 = SN_random(plt1)"
      ],
      "metadata": {
        "id": "MF35PBh8SkvC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(plt1)"
      ],
      "metadata": {
        "id": "QeuNqhbUSLQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(plt2)"
      ],
      "metadata": {
        "id": "a5rtwlPLSwxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NkFdmptzcPWJ",
        "ydJXq903cpDg",
        "2DGUb17oECQ_",
        "lQfxsIBCEIje",
        "tLnL3_d6EPKR",
        "AE5m2sGVEg6O",
        "No8M26AlEwHj",
        "0GPDIkGUdlAK",
        "XL1QS7uAeP2E",
        "Sy8BmsOperTK",
        "YJQvuH0kQo4v",
        "8EzKMWdpSKbN"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyOBIX2vM4V0mvAQnhce8UIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}